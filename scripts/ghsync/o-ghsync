#!/usr/bin/python3
# Ehi, emacs, this is -*- python-mode -*-

import argparse
import concurrent.futures
import logging
import os
import sys
import threading
import time
import warnings
from pathlib import Path
from urllib.parse import urljoin

# Suppress requests charset warning
warnings.filterwarnings("ignore", message="Unable to find acceptable character detection dependency")

import requests

# Add repo root to sys.path to allow imports from infra.
script_path = Path(__file__).resolve()
repo_root = None

# Traverse up to find the directory containing 'infra'
for parent in script_path.parents:
    if (parent / "infra").is_dir():
        repo_root = parent
        break

if repo_root:
    if str(repo_root) not in sys.path:
        sys.path.insert(0, str(repo_root))
else:
    # Fallback for when 'infra' might not be visible or we are in a different structure
    # Try standard depths if available
    potential_roots = []
    if len(script_path.parents) > 3:
        potential_roots.append(script_path.parents[3]) # infra/tools/ghsync/o-ghsync -> root
    if len(script_path.parents) > 2:
        potential_roots.append(script_path.parents[2]) # scripts/ghsync/o-ghsync -> root
    
    for root in potential_roots:
         if (root / "infra").exists():
             if str(root) not in sys.path:
                 sys.path.insert(0, str(root))
             break

try:
    from infra.tools.ghsync.utils.auth import get_password
    from infra.tools.ghsync.utils.db import (
        get_all_repositories,
        get_repository_id,
        get_sync_metadata,
        init_db,
        update_sync_metadata,
        upsert_issues,
        upsert_labels,
        upsert_repositories,
    )
except ImportError:
    # Fallback for detached execution (e.g. copied ghsync folder)
    # Add script directory to sys.path to allow relative imports
    script_dir = str(script_path.parent)
    if script_dir not in sys.path:
        sys.path.insert(0, script_dir)
        
    try:
        from utils.auth import get_password
        from utils.db import (
            get_all_repositories,
            get_repository_id,
            get_sync_metadata,
            init_db,
            update_sync_metadata,
            upsert_issues,
            upsert_labels,
            upsert_repositories,
        )
    except ImportError as e:
        print(f"Error importing modules: {e}")
        print("Please ensure you are running this script from the correct environment.")
        sys.exit(1)

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(threadName)s - %(message)s",
    handlers=[logging.StreamHandler(sys.stdout)],
)
logger = logging.getLogger(__name__)

# Lock for database writes (SQLite writes should be serialized)
db_lock = threading.Lock()


class GitHubClient:
    def __init__(self, token, max_retries=3, retry_delay=1.0, base_url="https://api.github.com"):
        self.token = token
        self.max_retries = max_retries
        self.retry_delay = retry_delay
        self.base_url = base_url
        self.session = requests.Session()
        self.session.headers.update({
            "Accept": "application/vnd.github+json",
            "Authorization": f"Bearer {token}",
            "X-GitHub-Api-Version": "2022-11-28",
        })

    def _request(self, method, url, params=None, headers=None):
        if not url.startswith("http"):
            url = urljoin(self.base_url, url)

        delay = self.retry_delay
        for attempt in range(self.max_retries + 1):
            try:
                response = self.session.request(method, url, params=params, headers=headers)
                
                # Handle rate limits
                if response.status_code == 403 and "x-ratelimit-remaining" in response.headers:
                    remaining = int(response.headers["x-ratelimit-remaining"])
                    if remaining == 0:
                        reset_time = int(response.headers.get("x-ratelimit-reset", 0))
                        sleep_time = max(0, reset_time - time.time()) + 1
                        logger.warning(f"Rate limit exceeded. Sleeping for {sleep_time} seconds.")
                        time.sleep(sleep_time)
                        continue # Retry

                if response.status_code in (429, 500, 502, 503, 504):
                    if attempt < self.max_retries:
                        logger.warning(f"Request failed with status {response.status_code}. Retrying in {delay}s...")
                        time.sleep(delay)
                        delay *= 2  # Exponential backoff
                        continue
                
                return response
            except requests.RequestException as e:
                if attempt < self.max_retries:
                    logger.warning(f"Request exception: {e}. Retrying in {delay}s...")
                    time.sleep(delay)
                    delay *= 2
                else:
                    raise

        raise Exception(f"Max retries exceeded for {url}")

    def get(self, url, params=None, headers=None):
        return self._request("GET", url, params=params, headers=headers)


def fetch_paginated(client, url, params=None, headers=None):
    """Generator that yields items from a paginated API."""
    while url:
        response = client.get(url, params=params, headers=headers)
        
        # params are only for the first request
        params = None

        if response.status_code == 304:
            return # Not modified

        response.raise_for_status()
        
        data = response.json()
        if isinstance(data, list):
            yield from data
        elif isinstance(data, dict): # Should not happen for lists usually, but just in case
            yield data

        if "next" in response.links:
            url = response.links["next"]["url"]
        else:
            url = None
        
        # Capture ETag from the first response if needed (passed back differently usually)
        # But here we yield items. Handling ETag for the whole collection is tricky with generators
        # unless we return the response object too.
        # For simplicity in this parallel version, we'll handle ETag outside or simplified.


def sync_repositories(client, user, db_conn, force=False):
    logger.info(f"Syncing repositories for {user}...")
    endpoint_name = "repos"
    
    etag = None
    if not force:
        metadata = get_sync_metadata(db_conn, user=user, repository="", endpoint=endpoint_name)
        etag = metadata.get("etag")

    headers = {}
    if etag:
        headers["If-None-Match"] = etag

    url = f"/orgs/{user}/repos"
    params = {'per_page': 100}
    
    all_repos = []
    first_etag = None
    
    # We can't use the simple generator above because we need the ETag from the first response
    # and we need to handle 304 specifically for the whole collection.
    
    current_url = url
    current_params = params
    
    while current_url:
        response = client.get(current_url, params=current_params, headers=headers)
        current_params = None # Clear params for next pages

        if not first_etag:
            first_etag = response.headers.get("ETag")

        if response.status_code == 304:
            logger.info(f"Repositories for {user} are not modified.")
            return

        response.raise_for_status()
        repos_data = response.json()
        all_repos.extend(repos_data)

        if "next" in response.links:
            current_url = response.links["next"]["url"]
        else:
            current_url = None

    if first_etag:
        with db_lock:
            update_sync_metadata(
                db_conn, user=user, repository="", endpoint=endpoint_name, etag=first_etag
            )

    if all_repos:
        with db_lock:
            upsert_repositories(db_conn, all_repos)
    
    logger.info(f"Synced {len(all_repos)} repositories for {user}.")
    return [r['full_name'] for r in all_repos]


def sync_repo_issues(client, repo_full_name, db_conn, force=False):
    owner, repo = repo_full_name.split("/")
    endpoint_name = "issues"
    
    # We need a new cursor for this thread if we were sharing connection without checks
    # But we are passing db_conn. If db_conn is shared across threads, we must rely on SQLite thread safety.
    # Python sqlite3 module: "check_same_thread" defaults to True.
    # We need to create a new connection per thread OR use one connection with check_same_thread=False (and serialize writes).
    # Since we use a ThreadPool, passing a single connection created in main thread (default) will fail.
    # We should pass the db_path and create a connection in the thread.
    
    # HOWEVER, the function signature provided in the prompt requirements didn't specify DB handling details.
    # I will adapt the worker to create its own connection.
    
    # Wait, the prompt says "db_conn" in existing code.
    # I'll change the signature to take db_path.
    pass

def sync_repo_task(db_path, token, repo_full_name, user, types, force, max_retries, retry_delay):
    # Each thread gets its own DB connection and Client
    try:
        conn = init_db(db_path)
        client = GitHubClient(token, max_retries, retry_delay)
        owner, repo = repo_full_name.split("/")

        if "issues" in types:
            _sync_issues(client, conn, owner, repo, force)
        
        if "labels" in types:
            _sync_labels(client, conn, owner, repo, force)
            
        conn.close()
    except Exception as e:
        logger.error(f"Error syncing {repo_full_name}: {e}", exc_info=True)


def _sync_issues(client, conn, owner, repo, force):
    endpoint_name = "issues"
    repo_full_name = f"{owner}/{repo}"
    
    # Read operation (get_repository_id)
    # SQLite allows concurrent reads.
    repository_id = get_repository_id(conn, repo_full_name)
    if not repository_id:
        logger.warning(f"Repository {repo_full_name} not found in DB. Skipping issues.")
        return

    etag = None
    since = None
    if not force:
        metadata = get_sync_metadata(conn, user=owner, repository=repo, endpoint=endpoint_name)
        etag = metadata.get("etag")
        since = metadata.get("last_sync_timestamp")

    headers = {}
    if etag:
        headers["If-None-Match"] = etag

    url = f"/repos/{owner}/{repo}/issues"
    params = {"per_page": 100, "state": "all"}
    if since:
        params["since"] = since

    all_issues = []
    first_etag = None
    
    current_url = url
    current_params = params
    
    while current_url:
        response = client.get(current_url, params=current_params, headers=headers)
        current_params = None

        if not first_etag:
            first_etag = response.headers.get("ETag")

        if response.status_code == 304:
            logger.info(f"Issues for {repo_full_name} not modified.")
            return

        response.raise_for_status()
        issues_data = response.json()
        all_issues.extend(issues_data)

        if "next" in response.links:
            current_url = response.links["next"]["url"]
        else:
            current_url = None

    # Write operations need lock
    with db_lock:
        if first_etag:
            update_sync_metadata(
                conn, user=owner, repository=repo, endpoint=endpoint_name, etag=first_etag
            )
        if all_issues:
            upsert_issues(conn, all_issues, repository_id)
    
    logger.info(f"Synced {len(all_issues)} issues for {repo_full_name}.")


def _sync_labels(client, conn, owner, repo, force):
    endpoint_name = "labels"
    repo_full_name = f"{owner}/{repo}"

    repository_id = get_repository_id(conn, repo_full_name)
    if not repository_id:
        logger.warning(f"Repository {repo_full_name} not found. Skipping labels.")
        return

    etag = None
    if not force:
        metadata = get_sync_metadata(conn, user=owner, repository=repo, endpoint=endpoint_name)
        etag = metadata.get("etag")

    headers = {}
    if etag:
        headers["If-None-Match"] = etag

    url = f"/repos/{owner}/{repo}/labels"
    params = {"per_page": 100}
    
    all_labels = []
    first_etag = None
    current_url = url
    current_params = params

    while current_url:
        response = client.get(current_url, params=current_params, headers=headers)
        current_params = None

        if not first_etag:
            first_etag = response.headers.get("ETag")

        if response.status_code == 304:
            logger.info(f"Labels for {repo_full_name} not modified.")
            return

        response.raise_for_status()
        labels_data = response.json()
        all_labels.extend(labels_data)

        if "next" in response.links:
            current_url = response.links["next"]["url"]
        else:
            current_url = None

    with db_lock:
        if first_etag:
            update_sync_metadata(
                conn, user=owner, repository=repo, endpoint=endpoint_name, etag=first_etag
            )
        if all_labels:
            upsert_labels(conn, all_labels, repository_id)

    logger.info(f"Synced {len(all_labels)} labels for {repo_full_name}.")


def main():
    parser = argparse.ArgumentParser(description="Standalone GitHub Sync Tool")
    parser.add_argument("--user", default="poly-repo", help="GitHub user or organization (default: poly-repo)")
    parser.add_argument("--repo", help="Specific repository to sync (optional)")
    parser.add_argument("--db", default=os.path.expanduser("~/.local/share/neo/workflow.sqlite"), help="Path to SQLite DB")
    parser.add_argument("--token", help="GitHub Personal Access Token (optional, defaults to auth lookup)")
    parser.add_argument("--workers", type=int, default=4, help="Number of parallel workers")
    parser.add_argument("--max-retries", type=int, default=3, help="Max retries for requests")
    parser.add_argument("--retry-delay", type=float, default=1.0, help="Initial retry delay in seconds")
    parser.add_argument("--force", action="store_true", help="Force full sync ignoring ETags")
    parser.add_argument("types", nargs="*", default=[], help="Data types to sync (issues, repos, labels). 'sync' is ignored if present.")
    parser.add_argument("--verbose", action="store_true", help="Enable debug logging")

    args = parser.parse_args()

    if args.verbose:
        logger.setLevel(logging.DEBUG)

    # Process types
    valid_types = {"issues", "repos", "labels"}
    selected_types = []
    
    # If no positional args provided, default to all
    raw_types = args.types if args.types else ["issues", "repos", "labels"]
    
    for t in raw_types:
        if t.lower() == "sync":
            continue
        if t.lower() in valid_types:
            selected_types.append(t.lower())
        else:
            logger.warning(f"Ignoring invalid type: {t}")
            
    if not selected_types:
        # Fallback if user only typed "sync"
        selected_types = ["issues", "repos", "labels"]
        
    logger.info(f"Syncing types: {selected_types}")

    # Resolve DB path
    db_path = Path(args.db)
    
    # Get Token
    token = args.token
    if not token:
        # Try to get from auth utils
        # Assuming the user arg is the 'login' for auth lookup or we use the user to find the password for 'api.github.com'
        # The existing code used: get_password('api.github.com', user + "^forge")
        # I'll stick to that convention if no token provided.
        token = get_password("api.github.com", args.user + "^forge")
        if not token:
            # Fallback/Retry without suffix if user provided just name
            token = get_password("api.github.com", args.user)
    
    if not token:
        logger.error("No GitHub token provided or found in .authinfo.gpg. Please provide --token.")
        sys.exit(1)

    # Initialize DB (Main thread)
    conn = init_db(db_path)
    client = GitHubClient(token, args.max_retries, args.retry_delay)

    # 1. Sync Repositories (Sequential or parallel? Usually fetching list of repos is one request chain)
    # We need to sync repos first to populate the DB for FKs.
    repos_to_sync = []
    
    if "repos" in selected_types:
        try:
            synced_repos = sync_repositories(client, args.user, conn, args.force)
            if args.repo:
                # Filter if user specified a repo
                full_name = f"{args.user}/{args.repo}"
                if full_name in synced_repos:
                    repos_to_sync = [full_name]
                else:
                    logger.warning(f"Repository {full_name} not found in synced repositories.")
            else:
                repos_to_sync = synced_repos
        except Exception as e:
            logger.error(f"Failed to sync repositories: {e}")
            sys.exit(1)
    else:
        # If we didn't sync repos, we need to fetch what's in DB or use args
        if args.repo:
            repos_to_sync = [f"{args.user}/{args.repo}"]
        else:
            repos_to_sync = get_all_repositories(conn)

    conn.close() # Close main thread connection before spawning threads (good practice with SQLite)

    # 2. Sync Issues and Labels in Parallel
    if ("issues" in selected_types or "labels" in selected_types) and repos_to_sync:
        logger.info(f"Syncing details for {len(repos_to_sync)} repositories with {args.workers} workers...")
        
        with concurrent.futures.ThreadPoolExecutor(max_workers=args.workers) as executor:
            futures = {
                executor.submit(
                    sync_repo_task, 
                    db_path, 
                    token, 
                    repo, 
                    args.user, 
                    selected_types, 
                    args.force, 
                    args.max_retries, 
                    args.retry_delay
                ): repo 
                for repo in repos_to_sync
            }
            
            for future in concurrent.futures.as_completed(futures):
                repo = futures[future]
                try:
                    future.result()
                except Exception as e:
                    logger.error(f"Worker failed for {repo}: {e}")

    logger.info("Sync completed.")

if __name__ == "__main__":
    main()
